{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabday/graph-neural-networks/blob/main/0_PointCloudSemanticSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PLEASE, MAKE A COPY OF THIS COLAB BEFORE RUNNING ANYTHING.\n",
        "\n",
        "import torch\n",
        "import os\n",
        "# Install required packages.\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q open3d\n",
        "\n"
      ],
      "metadata": {
        "id": "dTGQEqkTxcG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import logging\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from multiprocessing import cpu_count\n",
        "from plotly.subplots import make_subplots\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.transforms import NormalizeScale, FixedPoints\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.WARNING)"
      ],
      "metadata": {
        "id": "5aviBHcPsibV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function to set a random seed\n",
        "def seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(12345)\n",
        "\n",
        "# A helper function that loads the point cloud data and leaves only X,Y,Z coordinates with the point labels.\n",
        "def load_point_cloud(file_path: str) -> np.ndarray:\n",
        "    file_path = os.path.abspath(file_path)\n",
        "    if not file_path.endswith('.npy'):\n",
        "        raise ValueError(\"File must be a .npy file!\")\n",
        "\n",
        "    points = np.load(file_path)\n",
        "    points_labels = points[:, [0, 1, 2, -1]]\n",
        "    return points_labels\n",
        "\n",
        "# A helper visualization function that is versatile and can take either raw point clouds from a file or work directly with\n",
        "# PyG data objects. This function can visuailize either 1 or 2 point clouds side by side.\n",
        "def visualize(\n",
        "        point_cloud_fname1: Optional[str]=None,\n",
        "        point_cloud_fname2: Optional[str]=None,\n",
        "        point_cloud_graph1: Optional[Data]=None,\n",
        "        point_cloud_graph2: Optional[Data]=None,\n",
        "        edge_indices: Tuple[Optional[torch.Tensor], ...]=(None, None),\n",
        "        indices: Tuple[Optional[torch.Tensor], ...]=(None, None),\n",
        "        show_both: bool=False,\n",
        "        name1: Optional[str]=None,\n",
        "        name2: Optional[str]=None\n",
        ") -> None:\n",
        "    \"\"\"Visualize one or two point clouds, either from files or PyG Data objects, optionally displaying edge connections.\n",
        "\n",
        "    Args:\n",
        "        point_cloud_fname1 (Optional[str]): File name for the 1st point cloud. If given, overrides `point_cloud_graph1`.\n",
        "        point_cloud_fname2 (Optional[str]): File name for the 2nd point cloud. If given, overrides `point_cloud_graph2`.\n",
        "        point_cloud_graph1 (Optional[Data]): PyG Data object for the 1st point cloud.\n",
        "        Used if `point_cloud_fname1` is not given.\n",
        "        point_cloud_graph2 (Optional[Data]): PyG Data object for the 2nd point cloud.\n",
        "        Used if `point_cloud_fname2` is not given.\n",
        "        edge_indices (Tuple[Optional[torch.Tensor], ...]): Tuple containing edge indices for the point clouds.\n",
        "        Each entry should be a tensor of shape [2, num_edges] or None.\n",
        "        indices (Tuple[Optional[torch.Tensor], ...]): Tuple containing indices of points to highlight in the point clouds.\n",
        "        Each entry should be a tensor of indices or None.\n",
        "        show_both (bool): Whether to visualize one or two point clouds. If True, visualizes both point clouds side by side.\n",
        "        name1 (Optional[str]): Optional name for the first point cloud to use in the plot title.\n",
        "        name2 (Optional[str]): Optional name for the second point cloud to use in the plot title.\n",
        "    \"\"\"\n",
        "\n",
        "    if not any([point_cloud_fname1, point_cloud_fname2, point_cloud_graph1, point_cloud_graph2]):\n",
        "        logger.warning(\"Provide at least one point cloud file name or PyG Data object to visualize!\")\n",
        "        return\n",
        "\n",
        "    if (point_cloud_fname1 and point_cloud_graph1) or (point_cloud_fname2 and point_cloud_graph2):\n",
        "        logger.warning(\"Provide either a file with a specific point cloud or a PyG Data object, but not both\")\n",
        "        return None\n",
        "\n",
        "    def load_data(point_cloud_fname, point_cloud_graph):\n",
        "        if point_cloud_fname:\n",
        "            points_labels = load_point_cloud(point_cloud_fname)\n",
        "            pos = points_labels[:, :-1]\n",
        "            labels = points_labels[:, -1]\n",
        "        else:\n",
        "            pos = point_cloud_graph.pos\n",
        "            labels = point_cloud_graph.y\n",
        "        return pos, labels\n",
        "\n",
        "    pos1, labels1 = load_data(point_cloud_fname1, point_cloud_graph1)\n",
        "    pos2, labels2 = None, None\n",
        "    if show_both:\n",
        "        pos2, labels2 = load_data(point_cloud_fname2, point_cloud_graph2)\n",
        "\n",
        "    if show_both:\n",
        "        titles = [name if name else f\"Point Cloud {num}\" for num, name in enumerate([name1, name2])]\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=2,\n",
        "            specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
        "            subplot_titles=[\n",
        "                os.path.basename(point_cloud_fname1) if point_cloud_fname1 else titles[0],\n",
        "                os.path.basename(point_cloud_fname2) if point_cloud_fname2 else titles[1]\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        fig = go.Figure()\n",
        "\n",
        "    def add_scatter_traces(fig, pos, labels, row=None, col=None, edge_index=None, index=None):\n",
        "        if edge_index is not None:\n",
        "            for (src, dst) in edge_index.t().tolist():\n",
        "                src = pos[src].tolist()\n",
        "                dst = pos[dst].tolist()\n",
        "                fig.add_trace(\n",
        "                    go.Scatter3d(\n",
        "                        x=[src[0], dst[0]], y=[src[1], dst[1]], z=[src[2], dst[2]],\n",
        "                        mode='lines',\n",
        "                        line=dict(width=0.5, color='black'),\n",
        "                        opacity=0.5\n",
        "                    ),\n",
        "                    row=row, col=col\n",
        "                )\n",
        "        if index is None:\n",
        "            fig.add_trace(\n",
        "                go.Scatter3d(\n",
        "                    x=pos[:, 0], y=pos[:, 1], z=pos[:, 2],\n",
        "                    mode='markers',\n",
        "                    marker=dict(size=1.5, color=labels, colorscale=\"Viridis\")\n",
        "                ),\n",
        "                row=row, col=col\n",
        "            )\n",
        "        else:\n",
        "            mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
        "            mask[index] = True\n",
        "            fig.add_trace(\n",
        "                go.Scatter3d(\n",
        "                    x=pos[~mask, 0], y=pos[~mask, 1], z=pos[~mask, 2],\n",
        "                    mode='markers',\n",
        "                    marker=dict(size=1.5, color='lightgray')\n",
        "                ),\n",
        "                row=row, col=col\n",
        "            )\n",
        "            fig.add_trace(\n",
        "                go.Scatter3d(\n",
        "                    x=pos[mask, 0], y=pos[mask, 1], z=pos[mask, 2],\n",
        "                    mode='markers',\n",
        "                    marker=dict(size=1.5, color=labels[mask], colorscale=\"Viridis\")\n",
        "                ),\n",
        "                row=row, col=col\n",
        "            )\n",
        "\n",
        "    if show_both:\n",
        "        add_scatter_traces(fig, pos1, labels1, row=1, col=1, edge_index=edge_indices[0], index=indices[0])\n",
        "        add_scatter_traces(fig, pos2, labels2, row=1, col=2, edge_index=edge_indices[1], index=indices[1])\n",
        "    else:\n",
        "        add_scatter_traces(fig, pos1, labels1, edge_index=edge_indices[0], index=indices[0])\n",
        "\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis=dict(visible=False),\n",
        "            yaxis=dict(visible=False),\n",
        "            zaxis=dict(visible=False),\n",
        "            aspectmode='data'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if show_both:\n",
        "        fig.update_layout(\n",
        "            scene1=dict(\n",
        "                xaxis=dict(visible=False),\n",
        "                yaxis=dict(visible=False),\n",
        "                zaxis=dict(visible=False),\n",
        "                aspectmode='data'\n",
        "            ),\n",
        "            scene2=dict(\n",
        "                xaxis=dict(visible=False),\n",
        "                yaxis=dict(visible=False),\n",
        "                zaxis=dict(visible=False),\n",
        "                aspectmode='data'\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "UOHreQsj4n7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1JW9gZodzmC"
      },
      "source": [
        "# Semantic segmentation with 3D point clouds and GNNs\n",
        "\n",
        "\n",
        "#### Sources:\n",
        "\n",
        "The notebook material was partially taken and modified from:\n",
        "\n",
        "1. [PointNet++ paper](https://arxiv.org/abs/1706.02413)\n",
        "2. [PyG PointNet++ examples](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pointnet2_classification.py)\n",
        "3. [VKitti3D Dataset](https://github.com/VisualComputingInstitute/vkitti3D-dataset?tab=readme-ov-file)\n",
        "\n",
        "#### Additional material:\n",
        "- [Virtual KITTI 3D dataset and it's description](https://github.com/VisualComputingInstitute/vkitti3D-dataset?tab=readme-ov-file)\n",
        "- [The original PointNet++ paper](https://arxiv.org/abs/1706.02413)\n",
        "- [The PyG tutorial on point cloud classification](https://colab.research.google.com/drive/1D45E5bUK3gQ40YpZo65ozs7hg5l-eo_U?usp=sharing#scrollTo=xqw5fnp5O832)\n",
        "- [Good old Euler rotation matrices](https://en.wikipedia.org/wiki/Rotation_matrix)\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1J1kH7nTOZ5XwViS7Fu5Yb9xcD3nWpYZL\" width=\"500\" height=\"500\"/>\n",
        "    <br>\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mso-IPRCmxG3"
      },
      "source": [
        "**The goal of point cloud segmentation is to assign a specific label or category to every point in the cloud, effectively partitioning the point cloud into different meaningful regions or objects.**\n",
        "\n",
        "Let's kick off with a small exercise.\n",
        "\n",
        "## Exercise 1\n",
        "\n",
        "1. Download a mesh object that represents a bunny made by the Standford university. Run the following command in a new cell to download the bunny:\n",
        "\n",
        "    `!wget https://graphics.stanford.edu/~mdfisher/Data/Meshes/bunny.obj`\n",
        "\n",
        "2. Read the bunny object with `open3d` package into a mesh object (don't worry if you don't know what a mesh is, it is not important here). Use this command:\n",
        "\n",
        "    `bunny_mesh = o3d.io.read_triangle_mesh(\"bunny.obj\")`\n",
        "\n",
        "3. Use the function `mesh2cloud` provided below that transforms an open3D TriangleMesh object into a point cloud to get a *cloud bunny*. Choose something around 30K points:\n",
        "\n",
        "    ```python\n",
        "    def mesh2cloud(mesh_obj: o3d.geometry.TriangleMesh, num_points: int) -> np.array:\n",
        "        point_cloud = mesh_obj.sample_points_uniformly(number_of_points=num_points)\n",
        "        return np.asarray(point_cloud.points)\n",
        "    ```\n",
        "    \n",
        "   You will get a 2D numpy array of the shape: $$N \\times 3,$$\n",
        "   **where $N$ is the number of points in your point cloud and 3 represent the 3 spatial coordinates, X, Y, Z. So each row of the matrix is nothing more than a position of the point in space.**\n",
        "\n",
        "4. Assign to each point on the bunny's body a color that would gradually change from 0 to 1 for the whole bunny. Think of this color as an output of a neural network that segments out different body parts of a bunny and assigns a specific color to them. Use the following line to create the colors for each point and concatenate them as the 4th coordinate to the given ones, which are X, Y, Z.\n",
        "\n",
        "    `colors = np.linspace(0, 1, num_points)`\n",
        "\n",
        "   **hint:** `use np.concatenate()`, the resulting shape should be $N \\times 4$.\n",
        "\n",
        "5. **You main contibution goes here:**\n",
        "    - Permute in a random order all the **spatial** points in the point cloud, **leaving the colors unchanged**.\n",
        "        - **hint:** you can use `np.random.permutation` to help you permute the points.\n",
        "    - Visualize the original bunny and the bunny with permuted spatial coordinates, using the `visualize_point_clouds` function given in the end of the exercise description. You can start with visualizing just the original bunny, setting `show_both` argument to `False`.\n",
        "    \n",
        "Think about the following:\n",
        "\n",
        "- What \"quantity(s)\" is(are) permutation invariant in the setting of this toy experiment?\n",
        "- What types of invariances/equivariances you can think of that would be desirable for a neural network operating on the point clouds domain?\n",
        "\n",
        "    ```python\n",
        "    # Visualization function\n",
        "    def visualize_point_clouds(\n",
        "        points1: np.ndarray,\n",
        "        points2: np.ndarray,\n",
        "        title1: str,\n",
        "        title2: str,\n",
        "        show_both: bool=True\n",
        "    ):\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=2,\n",
        "            specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
        "            subplot_titles=[title1, title2]\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "                x=points1[:, 0], y=points1[:, 1], z=points1[:, 2],\n",
        "                mode='markers',\n",
        "                marker=dict(size=1.5, color=points1[:, -1], colorscale='Viridis'),\n",
        "                name=title1\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        if show_both:\n",
        "            fig.add_trace(\n",
        "                go.Scatter3d(\n",
        "                    x=points2[:, 0], y=points2[:, 1], z=points2[:, 2],\n",
        "                    mode='markers',\n",
        "                    marker=dict(size=1.5, color=points2[:, -1], colorscale='Viridis'),\n",
        "                    name=title2\n",
        "                ),\n",
        "                row=1, col=2\n",
        "            )\n",
        "\n",
        "        fig.update_layout(\n",
        "            scene1=dict(\n",
        "                xaxis=dict(visible=False),\n",
        "                yaxis=dict(visible=False),\n",
        "                zaxis=dict(visible=False),\n",
        "                aspectmode='data'\n",
        "            ),\n",
        "            scene2=dict(\n",
        "                xaxis=dict(visible=False),\n",
        "                yaxis=dict(visible=False),\n",
        "                zaxis=dict(visible=False),\n",
        "                aspectmode='data'\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "    ```\n",
        "\n",
        "\n",
        "Extra tiny fun for those who's finished the exercise and is getting bored:\n",
        "\n",
        "- Use Euler 3D rotation matrices to rotate the bunny around some axis and visualize the output. The will come in handy soon.\n",
        "\n",
        "    ```python\n",
        "    # Rotation matricies around X, Y or Z-axis, theta given in radians\n",
        "    def R_axis(theta: float, axis: str) -> np.ndarray:\n",
        "        axis = axis.lower()\n",
        "        assert axis in ['x', 'y', 'z'], \"Axis must be either x, y or z\"\n",
        "        if axis == 'x':\n",
        "            return np.array([\n",
        "                [1, 0, 0],\n",
        "                [0, np.cos(theta), -np.sin(theta)],\n",
        "                [0, np.sin(theta), np.cos(theta)]\n",
        "        ] )\n",
        "        if axis == 'y':\n",
        "            return np.array([\n",
        "                [np.cos(theta), 0, np.sin(theta)],\n",
        "                [0, 1, 0],\n",
        "                [-np.sin(theta), 0, np.cos(theta)]\n",
        "            ])\n",
        "        if axis == 'z':\n",
        "            return np.array([\n",
        "                [np.cos(theta), -np.sin(theta), 0],\n",
        "                [np.sin(theta), np.cos(theta), 0],\n",
        "                [0, 0, 1]\n",
        "            ])\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nrZT34ody9m"
      },
      "outputs": [],
      "source": [
        "# ..... YOUR CODE HERE ....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNtwpeuTuhui"
      },
      "source": [
        "## Real-life point clouds\n",
        "\n",
        "- Let's move on to a real-life example and consider a problem of object segmentation in self-driving cars that use a LiDAR (light detection and ranging) to understand the road situation around a car.\n",
        "- We will work with a tiny subset of the Virtual KITTI 3D dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wj715gwv5np"
      },
      "source": [
        "### Dataset exploration\n",
        "\n",
        "Let's download the dataset, visualize it and proceed with transforming it to a PyG dataset format."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(file_id: str, file_name: str, split, destination_dir: str=\"data\") -> None:\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    destination_dir = os.path.join(destination_dir, split, \"raw\")\n",
        "    if not os.path.exists(destination_dir):\n",
        "        os.makedirs(destination_dir)\n",
        "    destination_path = os.path.join(destination_dir, file_name)\n",
        "    gdown.download(url, destination_path, quiet=True)\n",
        "\n",
        "file_ids2file_names_splits = [\n",
        "    (\"1vHAkZLAqgqoIf9VowXUJLDoyA8JD6Zbs\", \"frame_0.npy\", \"train\"),\n",
        "    (\"19C0AciVSdZeZPeAurpBI8RsKOwqgKvkS\", \"frame_1.npy\", \"train\"),\n",
        "    (\"1Eo-ekBp06KSsJ4Au_PfTt22s7b5Wmxrr\", \"frame_2.npy\", \"train\"),\n",
        "    (\"1RPveTSlgBA38QoBlf_GQImXrHhqhfXhs\", \"frame_3.npy\", \"train\"),\n",
        "    (\"14Han3ESj_zmKJCKTlyKRtJgqsy1kWYeF\", \"frame_4.npy\", \"train\"),\n",
        "    (\"1Lmg2PBNWpkovzlURrhkksI1EU_ZwAeub\", \"frame_0.npy\", \"test\")\n",
        "]\n",
        "\n",
        "# Download the point clouds and save them as 5 frames\n",
        "[download_data(*file_id_file_name_split) for file_id_file_name_split in file_ids2file_names_splits]"
      ],
      "metadata": {
        "id": "CEsbFOxQ4i1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23389747-804d-4725-807e-ad6fd619adde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(\n",
        "    point_cloud_fname1=\"data/train/raw/frame_0.npy\",\n",
        "    show_both=False\n",
        ") # it will take around 1 minute"
      ],
      "metadata": {
        "id": "n4Gu7q7-fYY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather some info about the point clouds we have\n",
        "frames = [\"data/train/raw/\" + f\"frame_{num}.npy\" for num in range(5)]\n",
        "frames += [\"data/test/raw/\" + \"frame_0.npy\"]\n",
        "\n",
        "# Initialize a set for the labels of the points\n",
        "unique_labels_prev = set()\n",
        "\n",
        "for num, frame in enumerate(frames):\n",
        "    points_labels = load_point_cloud(frame)\n",
        "    unique_labels_curr = set(np.unique(points_labels[:, -1]))\n",
        "    print(f\"\\nShape of the first frame is: {points_labels.shape}\")\n",
        "    print(f\"The number of unique label categories for frame {num} is: {len(unique_labels_curr)}\")\n",
        "\n",
        "    if num > 0:\n",
        "        new_label = unique_labels_curr -unique_labels_prev\n",
        "        if new_label:\n",
        "            print(f\"A new label {new_label} appears in the current frame, which wasn't present before \\n\")\n",
        "    unique_labels_prev = unique_labels_curr\n",
        "\n",
        "    if num == len(frames) - 1:\n",
        "        print(f\"\\nThe labels present in the frames: {unique_labels_prev}\")\n"
      ],
      "metadata": {
        "id": "5eZfEmZCwR-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb_7w3Dn92rR"
      },
      "source": [
        "- The dataset consists of 90 frames of simulated road scenes, using the original KITTI dataset. We will work only with 6 frames.\n",
        "- Each point cloud represents a LiDAR signal, collected from a roof of an imaginery car that drives along the virtual streets.\n",
        "- Each frame has around 400K data points and 11 unique labels.\n",
        "- The total number of labels is 14 and given by the table below:\n",
        "\n",
        "| Label ID | Semantics  | RGB             | Color       |\n",
        "|----------|------------|-----------------|-------------|\n",
        "| 0  | Terrain          | [200, 90, 0]    | brown       |\n",
        "| 1  | Tree             | [0, 128, 50]    | dark green  |\n",
        "| 2  | Vegetation       | [0, 220, 0]     | bright green|\n",
        "| 3  | Building         | [255, 0, 0]     | red         |\n",
        "| 4  | Road             | [100, 100, 100] | dark gray   |\n",
        "| 5  | GuardRail        | [200, 200, 200] | bright gray |\n",
        "| 6  | TrafficSign      | [255, 0, 255]   | pink        |\n",
        "| 7  | TrafficLight     | [255, 255, 0]   | yellow      |\n",
        "| 8  | Pole             | [128, 0, 255]   | violet      |\n",
        "| 9  | Misc             | [255, 200, 150] | skin        |\n",
        "| 10 | Truck            | [0, 128, 255]   | dark blue   |\n",
        "| 11 | Car              | [0, 200, 255]   | bright blue |\n",
        "| 12 | Van              | [255, 128, 0]   | orange      |\n",
        "| 13 | Don't care       | [0, 0, 0]       | black       |\n",
        "\n",
        "What challenges can you forsee when you will be working with such dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw6GC-cv-Q_P"
      },
      "source": [
        "### Graph creation\n",
        "\n",
        "- Many times people do not know how to create a graph, using their data, so we will go through all the required steps here and create a graph dataset using PyG and our point clouds.\n",
        "- As the documentation of PyG says you **do not** have to create the dataset in this way, but it's one of the ways and it's pretty helpful when you work with larger datasets, especially those which don't fit into your RAM.\n",
        "- Even though our dataset is still small, we will create a \"Large\" Dataset object that is used for datasets that do not fit into memory so you could potentially use it for a real-life problem in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dmu4aKAW2cm"
      },
      "source": [
        "#### Logic of the Dataset creation:\n",
        "\n",
        "One can implement 4 important methods to create a PyG Dataset, however some can be skipped:\n",
        "\n",
        "1. `raw_file_names()`: A list of files in the `raw` directory which needs to be found in order to skip the download.\n",
        "\n",
        "2. `processed_file_names()`: A list of files in the `processed` directory which needs to be found in order to skip the processing.\n",
        "\n",
        "3. `download()`: Downloads raw data into `raw` directory. *We will skip this step, since we already downloaded the data manually.*\n",
        "\n",
        "4. `process()`: Processes raw data and saves it into the `processed` directory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyVKittiDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root: str, size, pre_transofrm: callable=NormalizeScale(), transform: callable=None, **kwargs):\n",
        "       \"\"\"TinyVKitti Dataset class.\n",
        "       Args:\n",
        "           root (str): Root directory where the dataset should be saved.\n",
        "           size (int): The number of total VKITTI frame files in the root directory.\n",
        "           transform (callable, optional): A function/transform that takes in a\n",
        "               :class:`~torch_geometric.data.Data` or\n",
        "               :class:`~torch_geometric.data.HeteroData` object and returns a\n",
        "               transformed version.\n",
        "               The data object will be transformed before every access.\n",
        "               (default: :obj:`None`)\n",
        "           pre_transform (callable, optional): A function/transform that takes in\n",
        "               a :class:`~torch_geometric.data.Data` or\n",
        "               :class:`~torch_geometric.data.HeteroData` object and returns a\n",
        "               transformed version.\n",
        "               The data object will be transformed before being saved to disk.\n",
        "               (default: :obj:`None`)\n",
        "       \"\"\"\n",
        "       pass"
      ],
      "metadata": {
        "id": "pE_R7pS7wVGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP_BdPHHZzqB"
      },
      "source": [
        "Let's decrease the number of points in the cloud, since otherwise it would be unmanagable to process in a reasonable amount of time on this machine.\n",
        "\n",
        "- We will use **FixedPoint** of PyG transform that randomly samples $N$ points from a point cloud."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_points_transform = FixedPoints(num=10_000, replace=False)\n",
        "\n",
        "train_dataset_full = TinyVKittiDataset(root=\"data/train/\", size=5, log=False)\n",
        "train_dataset = TinyVKittiDataset(root=\"data/train/\", size=5, transform=fixed_points_transform, log=False)\n",
        "test_dataset = TinyVKittiDataset(root=\"data/test/\", size=1, transform=fixed_points_transform, log=False)"
      ],
      "metadata": {
        "id": "wF8Hi9Zpwe74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(\n",
        "    point_cloud_graph1=train_dataset_full[0],\n",
        "    point_cloud_graph2=train_dataset[0],\n",
        "    show_both=True,\n",
        "    name1=\"Original point cloud\",\n",
        "    name2=\"Randomly downsampled point cloud\"\n",
        ")"
      ],
      "metadata": {
        "id": "x8UPZQ4why9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acVP6wd0fHF1"
      },
      "source": [
        "## PointNet++\n",
        "\n",
        "We will re-implement the **[PointNet++](https://arxiv.org/abs/1706.02413)** architecture, which was a pioneering work for modeling point clouds directly, using principles usually applied by CNNs.\n",
        "\n",
        "PointNet++ processes point clouds iteratively stacking together set abstraction (SA) layers. Each SA layer follows three phases: sampling, grouping and neighborhood feature aggregation.\n",
        "\n",
        "1. The **sampling phase** implements a pooling scheme suitable for point clouds with potentially different sizes and selects a set of points from input points, which defines the centroids of local regions.\n",
        "\n",
        "2. The **grouping phase** constructs a graph in which nearby points are connected. Typically, this is done via ball queries (which connects all points that are within a radius to the query point).\n",
        "\n",
        "3. The **neighborhood feature aggregation phase** executes a Graph Neural Network layer that for each point aggregates information from its direct neighbors (given by the graph constructed in the previous phase). This steps encodes the local region patterns into a feature vector.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1lLDeG2tRywCT0vRxA5mZbco0flNgvlZR\"/>\n",
        "    <br>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAhorvSivOAv"
      },
      "source": [
        "### Sampling phase\n",
        "\n",
        "Let's understand and visualize the sampling phase and cover some of the caveats.\n",
        "\n",
        "Sampling (or downsamplig) phase uses the **Farthest Point Sampling** method.\n",
        "\n",
        "Given an input point set $\\{ \\mathbf{p}_1, \\ldots \\mathbf{p}_n \\}$, FPS iteratively selects a subset of points such that the sampled points are furthest apart. FPS has the following desired qualities:\n",
        "\n",
        "- Compared with random sampling, this procedure is known to have better coverage of the entire point set.\n",
        "-  In contrast to CNNs that scan the vector space agnostic of data distribution, FPS generates receptive fields in a data dependent manner.\n",
        "\n",
        "PyG has a ready-to-use implementation of [`fps`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.fps), which takes in the positions of nodes and a sampling ratio, and returns the indices of nodes that have been sampled."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn.pool import fps\n",
        "\n",
        "graph = train_dataset[0]\n",
        "fps_index = fps(graph.pos, ratio=0.5)"
      ],
      "metadata": {
        "id": "-xignAaw3UaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(\n",
        "    point_cloud_graph1=graph,\n",
        "    point_cloud_graph2=graph,\n",
        "    indices=[None, fps_index],\n",
        "    show_both=True,\n",
        "    name1=\"Original point cloud\",\n",
        "    name2=\"FPS downsampled point cloud\"\n",
        ")"
      ],
      "metadata": {
        "id": "w9htZ9olh4UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx5MEXrNIZqj"
      },
      "source": [
        "### Grouping phase via dynamic graph generation\n",
        "\n",
        "The input for this phase:\n",
        "- a set of points of the size $N \\times (d + C)$, where $d$ is the number of spatial coordinates and $C$ is the number of extra features, which is zero in our case.\n",
        "- the coordinates of a set of centroids of size $N^{\\prime} \\times d$\n",
        "\n",
        "The output are groups of point sets of size $N^{\\prime} \\times K \\times (d + C)$, where\n",
        "\n",
        "- each group corresponds to a local region.\n",
        "- $K$ is the number of points in the neighborhood of centroid points. Note that $K$ varies across groups but the succeeding aggregation layer is able to convert a variable number of points into a fixed-length local region feature vector.\n",
        "\n",
        "The locality of a region is measured by the corresponding distance.\n",
        "- For example, in CNNs it's a Manhattan distance between the neighbouring pixels that corresponds to the kernel size.\n",
        "- In general, our point clouds come from metric spaces with some predefined metric that allows to measure distances, angles and lengths. In the simplest case one can use Euclidean distance and ignore the curvature of the space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy_bJnQtMUv-"
      },
      "source": [
        "## Exercise 2.\n",
        "\n",
        "The authors of the paper offer 2 ways to group the sets and construct graphs made of the points of the set.\n",
        "\n",
        "The first way is by using a simple **kNN** (k nearest neighbours) algorithm, which finds a fixed number of neighbouring points to a given point.\n",
        "\n",
        "The second way is a **Ball query** method that finds all the points that are within a given radius of a query point, but in practice one usually limits the number of points to be no more than $K$.\n",
        "\n",
        "1. For this exercise, please, create another small instance of the `TinyVKittiDataset`, using the `FixedPoint` transform with 10000 points:\n",
        "    - **hint:** check how we did it above.\n",
        "\n",
        "2. Get a graph object of type `Data`, using the 0th graph from this dataset:\n",
        "    - Sample 10 centroid points by applying the `fps` method.\n",
        "        - **hint:** calculate the required `fps` ratio to achieve this.\n",
        "    - Group centroids and their neighbours into sets of graphs by using kNN search and Ball query methods:\n",
        "        - For the kNN search you can use the [`knn`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.knn.html#torch_geometric.nn.pool.knn) method provided by PyG. Set the number of neighbours `k` equal to 32.\n",
        "        - For the Ball query you can use the [`radius`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.radius.html#torch_geometric.nn.pool.radius) function, which is also a part of PyG package. Set the radius equal to 0.1 and the `max_num_neighbors` equal 32.\n",
        "        - **hint:** use the `fps` indices for the centroids that you got from the previous step to choose the points for which you need to find the nearest neighbours. Ask me if in doubt!\n",
        "    - Both of the methods return a `torch.Tensor` as the output, however we need to have source and destination nodes separately (source indices will be our centroid indices), so please use the following and fill in the missing values yourself:\n",
        "    ```python\n",
        "    dest_idx, src_idx = knn(x=..., y=..., k=32)\n",
        "    ```\n",
        "\n",
        "3. Visualize both methods, using the `visualize` function provided above.\n",
        "    - Keep the `indices` argument equal to `[None, None]`\n",
        "    - Construct 2 `edge_index` tensors for both methods.\n",
        "        - **Important:** check the `dest_idx` tensor and note that when you indexed your centroid positions with the indices given by the `fps` and then applied the kNN and Ball query search, the resulting `dest_idx` tensor doesn't have the original indices of the sampled centroids, so you will need to get them back, using the indices that you received from `fps`.\n",
        "        - Stack the `src_idx` tensor with the `dest_idx`that you got from the step above to construct the `edge_index` tensor for both methods. Use `torch.stack([src_idx, your_final_dest_idx], dim=0)` method to achieve this.\n",
        "    - Create a tuple of `edge_indices`, using 2 `edge_index` tensors from the previous step and feed it to the `visualize` function.\n",
        "\n",
        "4. Questions to answer:\n",
        "    - What difference do you see between the visualizations of two methods?\n",
        "    - Based on the difference, can you guess which method is more desirable and why?\n",
        "    - What type of a graph did we get applying both procedures?\n",
        "        - **hint:** recall the graph types from the colab about graph theory basics.\n",
        "    - Play around with the Ball query radius and see what changes. Which radius would you use for which purposes?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn.pool import radius, knn\n",
        "\n",
        "#### YOUR CODE HERE ####\n"
      ],
      "metadata": {
        "id": "YHyISVa16uxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9WjWcp03-h2"
      },
      "source": [
        "### Neighbourhood feature aggregation phase\n",
        "\n",
        "- The input to this phase is $N^{\\prime}$ local regions with their centroids that we sampled at step 1 and converted to graphs at step 2. The input has the dimensions $N^{\\prime} \\times K \\times (d + C)$.\n",
        "- So each local region is represented by its centroid and the accumulated features of the centroid's neighbours.\n",
        "- The output is a feature tensor of shape $N^{\\prime} \\times (d + C)$\n",
        "- The coordinates of local points in the region are translated into a local frame of the centroid coordinates, essentially performing the following operation:\n",
        "\n",
        "$$\n",
        "x_i^j = x_i^j - \\hat{x}^j, \\, \\text{for} \\, j \\in \\{1, ..., d\\},\n",
        "$$\n",
        "  where $\\hat{x}$ is the coordinate of the centroid and $d$ is the space dimensionality, which is 3 in our case.\n",
        "\n",
        "\n",
        "The PointNet++ neighbourhood aggregation steps described above can be written as a simple **message passing scheme** defined via\n",
        "\n",
        "$$\n",
        "\\mathbf{h}^{(\\ell + 1)}_i = \\max_{j \\in \\mathcal{N}(i)} \\textrm{MLP} \\left( \\mathbf{h}_j^{(\\ell)}, \\mathbf{p}_j - \\mathbf{p}_i \\right)\n",
        "$$\n",
        "where\n",
        "* $\\mathbf{h}_i^{(\\ell)} \\in \\mathbb{R}^d$ denotes the hidden features of point $i$ in layer $\\ell$\n",
        "* $\\mathbf{p}_i \\in \\mathbb{R}^3$ denotes the position of point $i$.\n",
        "\n",
        "**The PointNet++ can be viewed as the basic building block for local pattern learning. By using relative coordinates together with point features we can capture point-to-point relations in the local region.**\n",
        "\n",
        "This time we will make use of the `MessagePassing` interface that helps us in **creating message passing graph neural networks** by automatically taking care of message propagation.\n",
        "Here, we need to define its `message` function, as well as  which aggregation scheme to use, *e.g.*, `aggr=\"max\"` (see [here](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html) for the accompanying tutorial):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBwx079CMn__"
      },
      "source": [
        "### MessagePassing class\n",
        "\n",
        "Let's have a closer look at the `MessagePassing` base class:\n",
        "\n",
        "We only need to define the following:\n",
        "\n",
        "- `MessagePassing(aggr=\"add\", flow=\"source_to_target\", node_dim=-2)`, which defines:\n",
        "    - the aggregation scheme to use (\"add\", \"mean\" or \"max\")\n",
        "    - the flow direction of message passing (either \"source_to_target\" or \"target_to_source\")\n",
        "    - the node_dim attribute indicates along which axis to propagate, default is -2, so we propagate between nodes of dimension `batch, num_nodes, num_node_features`.\n",
        "\n",
        "- `MessagePassing.message(...)` constructs messages to the node $i$ for each edge.\n",
        "    - the message is constructed from node $j$ to $i$, which in PyG terminology means `source_to_target` because $i$ is the the target node and $j$ is the source by convention.\n",
        "    - $i$ is our centroid node and all nodes $j$ are the neighbouring nodes we want to run the `MessagePassing` from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSSqMjYRS7ga"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import MessagePassing\n",
        "\n",
        "# +++++ WRITE TOGETHER +++++\n",
        "\n",
        "class PointNetLayer(MessagePassing):\n",
        "    def __init__(self, nn: torch.nn.Module):\n",
        "        pass\n",
        "\n",
        "    def forward(\n",
        "            self, x: Tuple[torch.Tensor, ...], pos: Tuple[torch.Tensor, ...], edge_index: torch.Tensor\n",
        "        ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the layer, by utilizing the self.propagate method of the base class.\n",
        "        Args:\n",
        "            x: A tuple of feature vectors representing the feature vectors of all neighbours and the centroids\n",
        "            of shape ([num_neighbour_nodes, in_channels], [num_centroids, in_channels])\n",
        "            pos: A tuple of positions of all neighbours and the centroids\n",
        "            of shape ([num_neighbour_nodes, 3], [num_centroids, 3]\n",
        "            edge_index: If the flow is `source_to_target`(default) then edge_index[0] are all the source\n",
        "            (neighbouring) nodes and edge_index[1] are the destination (centroids) nodes\n",
        "\n",
        "        As our PointNetLayer class inherits from the PyG MessagePassing parent class,\n",
        "        we simply need to call the `propagate()` function which starts the\n",
        "        message passing procedure: `message()` -> `aggregate()` -> `update()`.\n",
        "\n",
        "        The MessagePassing class handles most of the logic for the implementation.\n",
        "        To build custom GNNs, we only need to define our own `message()`,\n",
        "        `aggregate()`, and `update()` functions (We use default aggregate() and update() here).\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def message(self, x_j: torch.Tensor, pos_j: torch.Tensor, pos_i: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Creates a message from the neighbouring nodes to the centroids.\n",
        "\n",
        "        The arguments can be a bit tricky to understand: `message()` can take\n",
        "        any arguments that were initially passed to `propagate`. Additionally,\n",
        "        we can differentiate destination nodes and source nodes by appending\n",
        "        `_i` or `_j` to the variable name, e.g. for the node features `h`, we\n",
        "        can use `h_i` and `h_j`.\n",
        "\n",
        "        Tensors passed to method `propagate` can be mapped to the respective nodes `i` and `j`\n",
        "        by appending `_i` or `_j` to the variable name, .e.g. pos_i and pos_j.\n",
        "\n",
        "        The `message()` function constructs messages for each edge in the graph.\n",
        "        The indexing of the original node features `h` (or other node variables) is handled under\n",
        "        the hood by PyG.\n",
        "\n",
        "        Args:\n",
        "            x_j: defines the features of neighboring nodes, shape [num_edges, in_channels]\n",
        "            pos_j: defines the position of neighboring nodes, shape [num_edges, 3]\n",
        "            pos_i: defines the position of centroids, shape [num_edges, 3]\n",
        "\n",
        "        Returns an output of the neural network that creates a message from the neighbouring nodes to the centroids\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDuzIFDQKYU3"
      },
      "source": [
        "### Local and Global Set Abstraction layers\n",
        "\n",
        "- A Set Abstraction (SA) layer encompasses all the logic that we wrote above and implements 2 types of operations:\n",
        "    - feature aggregation on the node level (local SA)\n",
        "    - feature aggregation on the whole point cloud level (global SA).\n",
        "\n",
        "Let's implement both of those layers together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sww18z8baErr"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn.pool import global_max_pool\n",
        "\n",
        "# +++++ WRITE TOGETHER +++++\n",
        "\n",
        "class SAModule(torch.nn.Module):\n",
        "    def __init__(self, ratio, r, nn):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: Optional[torch.Tensor], pos: torch.Tensor, batch: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the SAModule that applies the following 3 operations:\n",
        "        downsampling, grouping, message passing\n",
        "\n",
        "        Args:\n",
        "            x: Node features of shape [num_nodes, 3 + num_hidden_channels]\n",
        "            pos: Node positions of shape [num_nodes, 3]\n",
        "            batch:A batch tensor which assigns each node to a specific graph.. Shape [1, num_nodes]\n",
        "\n",
        "        Returns updated node features for centroids, their positions and batch tensor that assigns each centroid to\n",
        "        a particular graph in the batch:\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class GlobalSAModule(torch.nn.Module):\n",
        "    def __init__(self, nn):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: torch.Tensor, pos: torch.Tensor, batch: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
        "        \"\"\"\n",
        "        Computes the graph level embeddings by utilizing the embeddings of the centroids and their positions\n",
        "\n",
        "        Args:\n",
        "            x: A tensor of centroid embeddings that we get from the SAModule of shape [num_centroids, F_x], where F_x\n",
        "            is the dimensionality of the embeddings.\n",
        "            pos: A tensor of positional embedgings (similar to x)\n",
        "            batch:  A batch tensor which assigns each node to a specific graph. Shape [1, num_nodes]\n",
        "\n",
        "        Returns a final global embedding for the each point cloud in the batch\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUk9y-Fbjedh"
      },
      "source": [
        "### Point Feature Propagation\n",
        "\n",
        "In SA layer, the original point set is downsampled. How do we obtain a label for each point in the cloud that we originally want to get?\n",
        "\n",
        "**The solution is to propagate features from the downsampled points to the original points, hence perform upsampling!**\n",
        "\n",
        "- We will propagate point features of dimension $N_l \\times (d +C)$ to $N_{l-1}$, where $N_l$ and $N_{l-1}$ are the point cloud sizes of the output and the input of set abstraction layer $l$, respectively.\n",
        "- $N_l \\leq N_{l-1}$ because of the downsampling.\n",
        "- The propagation is achieved by feature interpolation. The features are computed as the inverse distance weighted average for k nearest neighbours. The value $k$ is set to 3 in the paper. PyG has a corresponding method for that, [knn_interpolate](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.unpool.knn_interpolate.html), that implements the formula below:\n",
        "$$\n",
        "f(y) = \\frac{\\sum_{i=1}^{k}w(x_i)f(x_i)}{\\sum_{i=1}^kw(x_i)}, \\, \\text{where} \\, w(x_i) = \\frac{1}{d(\\textbf{p}(y), \\textbf{p}(x_i) )^2}, \\\\ \\text{and} \\,\n",
        "\\{x_1, ..., x_k\\} \\, \\text{are the k nearest neighbours},\n",
        "$$\n",
        "\n",
        "where $y$ are the coordinates of the input $N_{l-1}$ points and $f(y)$ are their features.\n",
        "- The interpolated features on $N_{l−1}$ points are then concatenated with a skipped connection given by the point features from the output of the corresponding set abstraction level.\n",
        "- Then the concatenated features are passed through a neural network to get more descriptive embeddings.\n",
        "- The process is repeated until the features are propagated to the original set of points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsTS7_qnjAP6"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn.unpool import knn_interpolate\n",
        "\n",
        "# +++++ WRITE TOGETHER +++++\n",
        "\n",
        "class FPModule(torch.nn.Module):\n",
        "    def __init__(self, k, nn):\n",
        "        pass\n",
        "\n",
        "    def forward(\n",
        "            self, x: torch.Tensor, pos: torch.Tensor, batch: torch.Tensor, x_skip: torch.Tensor,\n",
        "            pos_skip: torch.Tensor, batch_skip: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
        "        \"\"\"\n",
        "        Propagates features from the downsampled output of a SAModule back to the original points\n",
        "\n",
        "        Args:\n",
        "            x: A node feature tensor of shape [num_nodes/num_graphs, F_x]. We interpolate using these features!\n",
        "            pos: A node position tensor of shape [num_nodes, 3]. We interpolate using these positions!\n",
        "            batch: A batch tensor which assigns each node to a specific graph.\n",
        "\n",
        "            x_skip: A node feature tensor that we get aftert a specific SA/FP layer. [num_nodes, F_x]\n",
        "            pos_skip: A node position tensor with input node positions of the SA layer we want to upsample to.\n",
        "            These are the point positions we are interested in restoring and propagating the features to. We interpolate\n",
        "            using these positions!\n",
        "            batch_skip: A batch tensor which assigns each node from the output of a specific SA/FP layer to a specific\n",
        "            graph.\n",
        "\n",
        "        Returns upsampled node features, positions and a new batch tensor.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PointNet++ for point Segmentation\n",
        "\n",
        "Everything is ready to implement the full PointNet++ architectrure. Let's write a small network so it can be run in this colab, but one can always make it bigger.\n",
        "\n",
        "#### MLP class of PyG\n",
        "\n",
        "- PyG neatly implements an [MLP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MLP.html#torch_geometric.nn.models.MLP) class, which we need for SA and FP layers.\n",
        "- Instead of writing a MLP by hand we can use this class to compose any MLP with a ReLU nonlinearity as follows:\n",
        "\n",
        "  ```python\n",
        "  mlp = MLP([16, 32, 64, 128])\n",
        "  ```\n",
        "\n",
        "This notation above means that we created a 3-layers MLP neural network that has:\n",
        "\n",
        "- Layer-1 with 16 input channels and 32 output channels\n",
        "- Layer-2 with 32 input channelsand 64 output channels\n",
        "- Layer-3 with 64 input channels and 128 output channels\n",
        "\n",
        "One can also add dropout, a normalization function and other typical parameters."
      ],
      "metadata": {
        "id": "6bn-4RdLX5Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import MLP\n",
        "\n",
        "# +++++ WRITE TOGETHER +++++\n",
        "\n",
        "class PointNetPP(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        seed(12345)\n",
        "        pass"
      ],
      "metadata": {
        "id": "qcWhJttWUHRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "BpDQrO0lhVP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA or CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create a model instance\n",
        "model = PointNetPP(train_dataset.num_classes).to(device)\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create a loss criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "4fjG63baivkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "seed(12345)\n",
        "fixed_points_transform = FixedPoints(num=10_000, replace=False)\n",
        "train_dataset = TinyVKittiDataset(root=\"data/train/\", size=5, transform=fixed_points_transform, log=False)\n",
        "test_dataset = TinyVKittiDataset(root=\"data/test/\", size=1, transform=fixed_points_transform, log=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "Hzm0m93epDAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data.to(device)  # send tensors to GPU/CPU\n",
        "        optimizer.zero_grad()  # remove all grads from the previous step\n",
        "        logits = model(data)  # run inference\n",
        "        loss = criterion(logits, data.y)  # compute loss\n",
        "        loss.backward()  # compute gradients with respect to each model parameter\n",
        "        loss_all += loss.item() * data.num_graphs  # multiply loss by N graphs in a batch and add to the total loss\n",
        "        optimizer.step()  # apply grads and update the weights\n",
        "    return loss_all / len(train_loader.dataset)  # return avg loss per the whole dataset\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_nodes = 0\n",
        "    for data in loader:\n",
        "        data.to(device)\n",
        "        logits = model(data)\n",
        "        total_correct += logits.argmax(dim=1).eq(data.y).sum().item()\n",
        "        total_nodes += data.num_nodes\n",
        "\n",
        "    return total_correct / total_nodes\n",
        "\n"
      ],
      "metadata": {
        "id": "i32iZDEDoCu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# Training\n",
        "for epoch in range(1, 200):\n",
        "    loss = train(model, train_loader)\n",
        "    train_acc = test(model, train_loader)\n",
        "    test_acc = test(model, test_loader)\n",
        "    print(f'Epoch: {epoch}, Train loss: {loss :.4f}, Train acc: {train_acc :.4f}, Test acc: {test_acc :.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "ix_J5utClfTd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}